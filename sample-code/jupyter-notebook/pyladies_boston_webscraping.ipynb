{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1co_yFurBm2"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is a Jupyter Notebook adapted example of the sample code covered in the PyLadies Boston Webscraping presentation. The notebook is designed to run in Google Colab.\n",
        "\n",
        "The Selenium sample does not work in Google Colab due to permission limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39IGFZOAmzp8"
      },
      "source": [
        "# Prerequisite: Install Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_aDi084nNdu"
      },
      "source": [
        "## Step 1: Install Python dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7zNF-Z1m40E",
        "outputId": "d743ec78-1dc0-4fe7-e990-871bf41741a2"
      },
      "outputs": [],
      "source": [
        "!pip install crawl4ai Scrapy pandas bs4 selenium playwright requests nest-asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtTEnW5Uqus6"
      },
      "source": [
        "## Step 2: Install system dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqet47mfo4tP",
        "outputId": "c9f95a2c-4839-4cf3-9b8b-87c20ca2f4f1"
      },
      "outputs": [],
      "source": [
        "!playwright install-deps  # Activate playwright\n",
        "!playwright install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q1ymLFws1KZ"
      },
      "source": [
        "# Crawl4AI Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1RiA1_Y9tR4P",
        "outputId": "ec06d2b0-7b87-4368-e53f-85ad3d1cb7c8"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "from crawl4ai import *\n",
        "\n",
        "nest_asyncio.apply() # Needed for Jupyter Notebook only\n",
        "\n",
        "async def main():\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        result = await crawler.arun(\n",
        "            url=\"https://en.wikipedia.org/wiki/OpenAI\",\n",
        "        )\n",
        "        print(result.markdown)\n",
        "        print(result.pdf)\n",
        "\n",
        "asyncio.run(main())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRs0fX_XwN5t"
      },
      "source": [
        "# Pandas Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQcdEeskwbAw",
        "outputId": "a7bf1c9a-d1ee-4f61-818c-e76a743717a9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    # Reads every table in the webpage into a list of DataFrames\n",
        "    result = pd.read_html(\"https://en.wikipedia.org/wiki/OpenAI\")\n",
        "    print(\"First table:\")\n",
        "    print(result[0])\n",
        "    print(\"Second table:\")\n",
        "    print(result[1])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QJAVuUtwn1Q"
      },
      "source": [
        "# Beautiful Soup Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbpDgPb7wsPl",
        "outputId": "427b26a6-4ebc-49d1-edc8-68cb74b51f9c"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def main():\n",
        "    # Make HTTP request to Wikipedia to retrieve HTML\n",
        "    response = requests.get(\"https://en.wikipedia.org/wiki/OpenAI\")\n",
        "\n",
        "    # Load HTML into Beautiful Soup HTML parser\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Retrieve the first table using CSS Selectors\n",
        "    result = soup.select_one(\"#mw-content-text > div.mw-content-ltr.mw-parser-output > table.infobox.ib-company.vcard\")\n",
        "\n",
        "    # Print the result\n",
        "    print(result)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbqqsj9zw0lO"
      },
      "source": [
        "# Scrapy Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M04LILxAwy7U",
        "outputId": "73448275-64cb-4426-9a61-6172ff70a29a"
      },
      "outputs": [],
      "source": [
        "!scrapy startproject scrapy_sample # Create a new scrapy project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrK6yetBxbjC"
      },
      "source": [
        "## Create a Scrapy \"Spider\"\n",
        "Next, you will need to create a file under `my_project/spiders/` and add the following code:\n",
        "\n",
        "```python\n",
        "'''\n",
        "Step 1: Create a new Scrapy project\n",
        "\n",
        "scrapy startproject my_project\n",
        "\n",
        "Step 2: Create a new spider in my_project/spiders/ folder\n",
        "Step 3: Run the crawler\n",
        "\n",
        "cd my_project\n",
        "scrapy crawl first  # without saving\n",
        "scrapy crawl first -O first.json # with saving\n",
        "\n",
        "Step 4 (Optional): Update settings.py to limit visits\n",
        "\n",
        "DEPTH_LIMIT = 3\n",
        "\n",
        "'''\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class MyFirstSpider(scrapy.Spider):\n",
        "    name = \"first\"\n",
        "    start_urls = [\n",
        "        \"https://www.torontopubliclibrary.ca/search.jsp?Ntt=python&Ndrs=\",\n",
        "    ]\n",
        "    def clean(self, data):\n",
        "        if data is None:\n",
        "            return data\n",
        "        else:\n",
        "            return data.strip()\n",
        "    def parse(self, response):\n",
        "        for book in response.css(\"div.description\"): # div.description.small-9.medium-10.columns\n",
        "            yield {\n",
        "                \"title\": self.clean(book.css(\"div.title.align-top > a > span::text\").get()),\n",
        "                \"author\": self.clean(book.css(\"div.format-year > span::text\").get()),\n",
        "                \"year\": self.clean(book.css(\"div.format-year > div > strong > span.date::text\").get()),\n",
        "            }\n",
        "\n",
        "        next_page = response.css(\"#search-bar-bottom > div > ul > li.pagination-next > a::attr(href)\").get()\n",
        "        if next_page is not None:\n",
        "            yield response.follow(next_page, callback=self.parse)\n",
        "```\n",
        "\n",
        "## Update Scrapy settings\n",
        "After, you need to edit `scrapy_sample/scrapy_sample/settings.py` and replace the content with the following:\n",
        "\n",
        "```python\n",
        "# Scrapy settings for scrapy_sample project\n",
        "#\n",
        "# For simplicity, this file contains only settings considered important or\n",
        "# commonly used. You can find more settings consulting the documentation:\n",
        "#\n",
        "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
        "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "\n",
        "BOT_NAME = \"scrapy_sample\"\n",
        "\n",
        "SPIDER_MODULES = [\"scrapy_sample.spiders\"]\n",
        "NEWSPIDER_MODULE = \"scrapy_sample.spiders\"\n",
        "\n",
        "ADDONS = {}\n",
        "\n",
        "\n",
        "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
        "#USER_AGENT = \"scrapy_sample (+http://www.yourdomain.com)\"\n",
        "\n",
        "# Obey robots.txt rules\n",
        "ROBOTSTXT_OBEY = False # changed for presentation\n",
        "DEPTH_LIMIT = 3 # changed for presentation\n",
        "\n",
        "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
        "#CONCURRENT_REQUESTS = 32\n",
        "\n",
        "# Configure a delay for requests for the same website (default: 0)\n",
        "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
        "# See also autothrottle settings and docs\n",
        "#DOWNLOAD_DELAY = 3\n",
        "# The download delay setting will honor only one of:\n",
        "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
        "#CONCURRENT_REQUESTS_PER_IP = 16\n",
        "\n",
        "# Disable cookies (enabled by default)\n",
        "#COOKIES_ENABLED = False\n",
        "\n",
        "# Disable Telnet Console (enabled by default)\n",
        "#TELNETCONSOLE_ENABLED = False\n",
        "\n",
        "# Override the default request headers:\n",
        "#DEFAULT_REQUEST_HEADERS = {\n",
        "#    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "#    \"Accept-Language\": \"en\",\n",
        "#}\n",
        "\n",
        "# Enable or disable spider middlewares\n",
        "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
        "#SPIDER_MIDDLEWARES = {\n",
        "#    \"scrapy_sample.middlewares.ScrapySampleSpiderMiddleware\": 543,\n",
        "#}\n",
        "\n",
        "# Enable or disable downloader middlewares\n",
        "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
        "#DOWNLOADER_MIDDLEWARES = {\n",
        "#    \"scrapy_sample.middlewares.ScrapySampleDownloaderMiddleware\": 543,\n",
        "#}\n",
        "\n",
        "# Enable or disable extensions\n",
        "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
        "#EXTENSIONS = {\n",
        "#    \"scrapy.extensions.telnet.TelnetConsole\": None,\n",
        "#}\n",
        "\n",
        "# Configure item pipelines\n",
        "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
        "#ITEM_PIPELINES = {\n",
        "#    \"scrapy_sample.pipelines.ScrapySamplePipeline\": 300,\n",
        "#}\n",
        "\n",
        "# Enable and configure the AutoThrottle extension (disabled by default)\n",
        "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
        "#AUTOTHROTTLE_ENABLED = True\n",
        "# The initial download delay\n",
        "#AUTOTHROTTLE_START_DELAY = 5\n",
        "# The maximum download delay to be set in case of high latencies\n",
        "#AUTOTHROTTLE_MAX_DELAY = 60\n",
        "# The average number of requests Scrapy should be sending in parallel to\n",
        "# each remote server\n",
        "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
        "# Enable showing throttling stats for every response received:\n",
        "#AUTOTHROTTLE_DEBUG = False\n",
        "\n",
        "# Enable and configure HTTP caching (disabled by default)\n",
        "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
        "#HTTPCACHE_ENABLED = True\n",
        "#HTTPCACHE_EXPIRATION_SECS = 0\n",
        "#HTTPCACHE_DIR = \"httpcache\"\n",
        "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
        "#HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n",
        "\n",
        "# Set settings whose default value is deprecated to a future-proof value\n",
        "FEED_EXPORT_ENCODING = \"utf-8\"\n",
        "```\n",
        "## Run the crawler\n",
        "\n",
        "Lastly, run the crawler!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lne1v_7exl7o",
        "outputId": "4fc2c6d5-432d-4a7e-a1e8-b83fca0acfad"
      },
      "outputs": [],
      "source": [
        "!cd scrapy_sample\n",
        "!scrapy crawl first # run the crawler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDGK-KzHyw-f"
      },
      "source": [
        "# Selenium Sample\n",
        "\n",
        "NOTE: This example does not work in Colab. You will need to run Jupyter Notebook locally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "JZJtnf5qy50R",
        "outputId": "22e5fae1-fcc7-404e-cd71-8cfe4978d751"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "def main():\n",
        "    # Use Chrome to visit webpages\n",
        "    driver = webdriver.Chrome() # Will not work in Colab\n",
        "\n",
        "    # Visit OpenAI webpage\n",
        "    driver.get(\"https://en.wikipedia.org/wiki/OpenAI\")\n",
        "\n",
        "    # Find link (anchor) element that goes to Sam Altman's Wiki page using XPATH\n",
        "    sam_altman_link_element = driver.find_element(By.XPATH, '//*[@id=\"mw-content-text\"]/div[1]/table[1]/tbody/tr[5]/td/div/ul/li[1]/a')\n",
        "\n",
        "    # Visit Sam Altman's wiki page\n",
        "    sam_altman_link_element.click()\n",
        "\n",
        "    # Check what the current page is\n",
        "    current_url = driver.current_url\n",
        "    print(f\"We are currently on: {current_url}\")\n",
        "\n",
        "    # Print content of current page (can be passed to Beautiful Soup HTML parser)\n",
        "    print(f\"{driver.page_source[0:1000]}...\")\n",
        "\n",
        "    # Close driver\n",
        "    driver.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
